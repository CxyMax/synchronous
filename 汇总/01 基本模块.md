**01. 自动微分**

训练神经网络通常包含一次前向传递和一次反向传递，前向传递神经网络会计算并保留节点计算值，在反向传递时用于更新神经网络参数。

<img src="../../md_imgs/image-20241128104213459.png" alt="image-20241128104213459" style="zoom: 80%;" />

**02. nn. Module**

自己定义网络模块一般继承nn.Module类并重写\__init__()和forward()方法，nn.Module常用函数：

**`.to(device)`**: Moves the model to the specified device (e.g., CPU or GPU).

**`.train()`**: Sets the model to training mode.

**`.eval()`**: Sets the model to evaluation mode (affects layers like `Dropout`, `BatchNorm`).

**`.parameters()`**: Returns an iterator over the model’s parameters (weights).

**`.zero_grad()`**: Zeroes the gradients of all model parameters.

**`.state_dict()`**: Returns a dictionary of the model’s parameters (weights and biases).

**`.load_state_dict()`**: Loads the model parameters from a saved dictionary.



**03. Softmax**

如果您在训练期间使用 nn.CrossEntropyLoss 作为损失函数，则无需向网络添加 Softmax 层。这是因为 nn.CrossEntropyLoss 将 nn.LogSoftmax() 和 nn.NLLLoss() （负对数似然损失）组合在一个类别中。它需要原始 logits（在您的情况下为 nn.Linear(784, 10) 的输出）并在内部应用 LogSoftmax。



**04. BN**

BN的位置：
1. BN用于全连接, Linear->BN->ReLU()
2. BN用于卷积，卷积层 (Conv Layer) → 归一化层 (BatchNorm) → 激活函数 (ReLU) → 池化层 (Pooling)

BN作用
1. 因为BN层将激活值进行了归一化处理，降低了激活值变得很大或很小的风险，设置大一点的学习率可以加快收敛速度。
2. 均值和方差是在不同Batch间调整的，算作一种噪音，加强了模型泛化能力

```python
# BN在训练时和预测时的行为不一样
# 训练时的均值为本批次的均值，预测的均值为全局均值

def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    
    if not torch.is_grad_enabled():
        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        # 如果为全连接层输入
        if len(X.shape) == 2:
            # 均值为一个Batch的样本在不同特征上的均值组成的向量, mean.shape=torch.Size([feature_numbers])
            mean = X.mean(dim=0)
            # 同理方差也为一个Batch的样本在不同特征上的方差组成的向量, var.shape=torch.Size([feature_numbers])
            var = ((X - mean) ** 2).mean(dim=0)
        # 如果为卷积层输入
        else:
            # mean.shape=var.shape=torch.Size([1,channels,1,1])
            mean = X.mean(dim=(0, 2, 3), keepdim=True)
            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)
        # 利用本batch的均值和方差做归一化
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 全局的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    # gamma.shape=beta.shape=torch.Size([1,6,1,1]) ?
    Y = gamma * X_hat + beta
    # 原代码写的return Y, moving_mean.detach(), moving_var.detach()
    # 感觉没必要，因为两者都是不带梯度的
    return Y, moving_mean, moving_var
```

```python
class BN(nn.Module):
    # 这里kwargs的作用时向父类传递可能用到的参数
    def __init__(self,features,dimensions,**kwargs):
        super().__init__(**kwargs)
        if dimensions == 2:
            self.shape = (1,features)
        else:
            self.shape = (1,features,1,1)
        # gamma 和 beta 为可学习参数. requires_grad=True
        self.gamma = nn.Parameter(torch.ones(self.shape))
        self.beta = nn.Parameter(torch.zeros(self.shape))
        
        self.momentum = 0.9
        self.moving_mean = torch.zeros(self.shape)
        self.moving_var = torch.ones(self.shape)
        
    def forward(self,X):
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 保存更新过的moving_mean和moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=self.momentum)
        return Y
```

